{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c61400-dfa9-41b7-8971-772a0852fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame head:\n",
      "    token  gematria\n",
      "0  בראשית       913\n",
      "1     ברא       203\n",
      "2   אלהים        46\n",
      "3      את       401\n",
      "4   השמים       355 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with gematria=207:\n",
      "    token  gematria\n",
      "10   אור       207\n",
      "12   אור       207 \n",
      "\n",
      "Nearest neighbors for token at index 0:\n",
      "Query Token: 'בראשית' (Index=0)\n",
      "--------------------------------------------------\n",
      "1. 'בראשית' | Gematria=913 | Similarity=1.000\n",
      "2. 'ואת' | Gematria=407 | Similarity=0.999\n",
      "3. 'את' | Gematria=401 | Similarity=0.999\n",
      "4. 'את' | Gematria=401 | Similarity=0.999\n",
      "5. 'השמים' | Gematria=355 | Similarity=0.999\n"
     ]
    }
   ],
   "source": [
    "# SINGLE-CELL, HIGH-LEVEL DEMO\n",
    "\n",
    "# --- 1) Imports and Installs (if needed) ---\n",
    "# Comment these out if packages are already installed\n",
    "# !pip install transformers torch hebrew-tokenizer scikit-learn\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For Hebrew BERT\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --- 2) Define a small Hebrew text inline or load your own ---\n",
    "lines = [\n",
    "    \"בראשית ברא אלהים את השמים ואת הארץ\",\n",
    "    \"ויאמר אלהים יהי אור ויהי אור\",\n",
    "    \"וירא אלהים את האור כי טוב\"\n",
    "]\n",
    "\n",
    "# --- 3) Basic Cleaning and Normalization ---\n",
    "def clean_hebrew_text(lines):\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        norm_line = unicodedata.normalize('NFKC', line)\n",
    "        # Remove niqqud/diacritics\n",
    "        norm_line = re.sub(r'[\\u0591-\\u05C7]', '', norm_line)\n",
    "        cleaned_lines.append(norm_line.strip())\n",
    "    return cleaned_lines\n",
    "\n",
    "cleaned = clean_hebrew_text(lines)\n",
    "\n",
    "# --- 4) Gematria Logic ---\n",
    "GEMATRIA_MAP = {\n",
    "    'א': 1, 'ב': 2, 'ג': 3, 'ד': 4, 'ה': 5, 'ו': 6, 'ז': 7, 'ח': 8, 'ט': 9,\n",
    "    'י': 10, 'כ': 20, 'ל': 30, 'מ': 40, 'נ': 50, 'ס': 60, 'ע': 70, 'פ': 80, 'צ': 90,\n",
    "    'ק': 100, 'ר': 200, 'ש': 300, 'ת': 400\n",
    "}\n",
    "\n",
    "def calculate_gematria(word):\n",
    "    return sum(GEMATRIA_MAP.get(ch, 0) for ch in word)\n",
    "\n",
    "def simple_tokenize_hebrew(line):\n",
    "    return line.split()\n",
    "\n",
    "def create_token_dataframe(cleaned_lines):\n",
    "    rows = []\n",
    "    for line in cleaned_lines:\n",
    "        tokens = simple_tokenize_hebrew(line)\n",
    "        for t in tokens:\n",
    "            g_val = calculate_gematria(t)\n",
    "            rows.append({\"token\": t, \"gematria\": g_val})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_tokens = create_token_dataframe(cleaned)\n",
    "\n",
    "print(\"DataFrame head:\")\n",
    "print(df_tokens.head(), \"\\n\")\n",
    "\n",
    "# --- 5) (Optional) Load Hebrew BERT and Generate Embeddings ---\n",
    "def load_hebrew_transformer(model_name=\"onlplab/alephbert-base\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_semantic_embeddings(df_tokens, tokenizer, model, batch_size=16):\n",
    "    all_embeddings = []\n",
    "    tokens_list = df_tokens['token'].tolist()\n",
    "    for i in range(0, len(tokens_list), batch_size):\n",
    "        batch_tokens = tokens_list[i:i+batch_size]\n",
    "        encoded = tokenizer(batch_tokens, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "        # Take the [CLS] embedding\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_tokens = hidden_states[:, 0, :]\n",
    "        for row in cls_tokens:\n",
    "            all_embeddings.append(row.tolist())\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "tokenizer, model = load_hebrew_transformer()\n",
    "embeddings = generate_semantic_embeddings(df_tokens, tokenizer, model)\n",
    "\n",
    "# Combine gematria as an extra dimension in the embedding\n",
    "combined_vecs = []\n",
    "for i, row in df_tokens.iterrows():\n",
    "    gem = float(row['gematria'])\n",
    "    emb = embeddings[i]\n",
    "    # Append gematria to the embedding\n",
    "    combined_vecs.append(list(emb) + [gem])\n",
    "combined_vecs = np.array(combined_vecs)\n",
    "\n",
    "# --- 6) Demo: Find tokens with specific gematria ---\n",
    "def find_tokens_by_gematria(df_tokens, value):\n",
    "    return df_tokens[df_tokens[\"gematria\"] == value]\n",
    "\n",
    "target_val = 207  # e.g., often corresponds to \"אור\"\n",
    "matches = find_tokens_by_gematria(df_tokens, target_val)\n",
    "print(f\"Tokens with gematria={target_val}:\\n\", matches, \"\\n\")\n",
    "\n",
    "# --- 7) Demo: Nearest Neighbors Based on Embeddings ---\n",
    "def find_nearest_neighbors(token_idx, embeddings, top_k=5):\n",
    "    query_vec = embeddings[token_idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(query_vec, embeddings)[0]\n",
    "    nn_indices = np.argsort(sims)[::-1]  # descending\n",
    "    return nn_indices[:top_k], sims[nn_indices[:top_k]]\n",
    "\n",
    "def print_similar_tokens(df_tokens, embeddings, token_idx, top_k=5):\n",
    "    nn_indices, scores = find_nearest_neighbors(token_idx, embeddings, top_k)\n",
    "    query_token = df_tokens.iloc[token_idx][\"token\"]\n",
    "    print(f\"Query Token: '{query_token}' (Index={token_idx})\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    for rank, (idx, score) in enumerate(zip(nn_indices, scores), start=1):\n",
    "        t = df_tokens.iloc[idx]['token']\n",
    "        gem = df_tokens.iloc[idx]['gematria']\n",
    "        print(f\"{rank}. '{t}' | Gematria={gem} | Similarity={score:.3f}\")\n",
    "\n",
    "if len(df_tokens) > 0:\n",
    "    print(\"Nearest neighbors for token at index 0:\")\n",
    "    print_similar_tokens(df_tokens, combined_vecs, token_idx=0, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a4147-96dc-4db5-a986-f813c281dd70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
